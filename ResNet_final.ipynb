{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "gus94d284a1M"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v--MUSJd4a1c"
   },
   "source": [
    "# ResNet\n",
    "ResNet is the models that first introduced residual connections (a form of skip connections). It is a rather simple, but successful and very popular architecture. In this demo the [original version](https://arxiv.org/abs/1512.03385) for CIFAR-10 is re-implemented step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqZR5W2E4a1c"
   },
   "source": [
    "This is just a convenience function to make e.g. `nn.Sequential` more flexible. It is e.g. useful in combination with `x.squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "3PWJh_BX4a1f"
   },
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQGu4DQW4a1g"
   },
   "source": [
    "We begin by implementing the residual blocks. \n",
    "\n",
    "Note that we use 'SAME' padding, no bias, and batch normalization after each convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "rgb0Umej4a1h"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual block used by ResNet.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
    "        out_channels: The number of channels after the first convolution\n",
    "        stride: Stride size of the first convolution, used for downsampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()        \n",
    "        if stride > 1 or in_channels != out_channels:\n",
    "            # Add strides in the skip connection and zeros for the new channels.\n",
    "            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride],\n",
    "                                               (0, 0, 0, 0, 0, out_channels - in_channels),\n",
    "                                               mode=\"constant\", value=0))\n",
    "        else:\n",
    "            self.skip = nn.Sequential()\n",
    "            \n",
    "        # Initialize the required layers\n",
    "        # 2 CNNs with Kernel = 3 and SAME padding 3%2=1 (K%2)\n",
    "        self.stride = stride\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size=3,  \n",
    "                              stride = self.stride, padding=1)\n",
    "     \n",
    "        #second convolution doesn't have a stride size\n",
    "        self.conv2 = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, \n",
    "                               kernel_size=3, padding=1)\n",
    "        \n",
    "        # initialize batch normalization for each convolution\n",
    "        # number of interesting features equal to the outcoming channels \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Execute the required layers and functions\n",
    "        \n",
    "        # initialize skip connection\n",
    "        skip = self.skip(input)\n",
    "        \n",
    "        # save forward pass of inner residual block in output \n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = output + skip\n",
    "        output = F.relu(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH_xXQo14a1j"
   },
   "source": [
    "Next we implement a stack of residual blocks for convenience. The first layer in the block is the one changing the number of channels and downsampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "3KAEOCrW4a1k"
   },
   "outputs": [],
   "source": [
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual blocks.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
    "        out_channels: The number of channels after the first layer\n",
    "        stride: Stride size of the first layer, used for downsampling\n",
    "        num_blocks: Number of residual blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize the required layers (blocks)\n",
    "        self.ResStack = nn.ModuleList()\n",
    "        self.first_block = True\n",
    "        self.stride = stride\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            if i == 0 and self.first_block==True:\n",
    "                self.ResStack.append(ResidualBlock(in_channels, out_channels,\n",
    "                self.stride))\n",
    "                \n",
    "                self.first_block = False\n",
    "            else:\n",
    "                self.ResStack.append(ResidualBlock(out_channels, out_channels))\n",
    "      \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Execute the layers (blocks)\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "\n",
    "        for i,l in enumerate(self.ResStack):\n",
    "            input = l(input)\n",
    "        \n",
    "        return input\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp8WIX3-4a1m"
   },
   "source": [
    "Now we implement the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "lzSwItfX4a1n"
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "num_classes = 10\n",
    "\n",
    "# Implement ResNet via nn.Sequential\n",
    "stride = 2 \n",
    "\n",
    "# used the information about the filters and the 10-way fully connected layer\n",
    "# to assume the input/output channels sizes after each residual stack\n",
    "resnet = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    ResidualStack(16,32, stride,n),\n",
    "    ResidualStack(32,64, stride,n),\n",
    "    ResidualStack(64,10, stride,n), \n",
    "    nn.AdaptiveAvgPool2d(1), #this is the size of the window, if set to 1 we are basically done with convolutions\n",
    "    Lambda(lambda x: x.squeeze()), \n",
    "    nn.Softmax(),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdEbDLjV4a1o"
   },
   "source": [
    "Next we initialize the weights of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "Bty1ANtE4a1p"
   },
   "outputs": [],
   "source": [
    "def initialize_weight(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        \n",
    "resnet.apply(initialize_weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KffRjcMA4a1p"
   },
   "source": [
    "# Training\n",
    "Now it is time to train the model.\n",
    "\n",
    "First we load the data and split them into train and validation set, so that we only use the test set when we are completely done developing our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "6J11FNS64a1q"
   },
   "outputs": [],
   "source": [
    "class CIFAR10Subset(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    Get a subset of the CIFAR10 dataset, according to the passed indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, idx=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if idx is None:\n",
    "            return\n",
    "        \n",
    "        self.data = self.data[idx]\n",
    "        targets_np = np.array(self.targets)\n",
    "        self.targets = targets_np[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaurbUGi4a1r"
   },
   "source": [
    "We next define transformations that change the images into PyTorch tensors, standardize the values according to the precomputed mean and standard deviation, and provide data augmentation for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "YmOTUWan4a1r"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQ1TwXox4a1s",
    "outputId": "26508ca3-2f7e-433c-c9d9-367bdfb9562e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "ntrain = 45_000\n",
    "train_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain),\n",
    "                          download=True, transform=transform_train)\n",
    "val_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain, 50_000),\n",
    "                        download=True, transform=transform_eval)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "lMEkeyNU4a1u"
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(train_set, batch_size=128,\n",
    "                                                   shuffle=True, num_workers=2,\n",
    "                                                   pin_memory=True)\n",
    "dataloaders['val'] = torch.utils.data.DataLoader(val_set, batch_size=128,\n",
    "                                                 shuffle=False, num_workers=2,\n",
    "                                                 pin_memory=True)\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(test_set, batch_size=128,\n",
    "                                                  shuffle=False, num_workers=2,\n",
    "                                                  pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5W9nE3M4a1u"
   },
   "source": [
    "Next we push the model to our GPU (if there is one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "vcIOFh_h4a1v"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "resnet.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNpPLE0Z4a1v"
   },
   "source": [
    "Next we define a helper method that does one epoch of training or evaluation. We have only defined training here, so you need to implement the necessary changes for evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "HTWn-M7U4a1w"
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, dataloader, train):\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        dataloader: Dataloader providing the data to run our model on\n",
    "        train: Whether this epoch is used for training or evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Set model to training mode (for e.g. batch normalization, dropout)\n",
    "    if train is True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    # Iterate over data\n",
    "    for xb, yb in dataloader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        if train ==True:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        with torch.set_grad_enabled(True):\n",
    "            pred = model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "            \n",
    "            top1 = torch.argmax(pred, dim=1)\n",
    "            ncorrect = torch.sum(top1 == yb)\n",
    "            \n",
    "            if train ==True:\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += ncorrect.item()\n",
    "    \n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    epoch_acc /= len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djhe27e44a1x"
   },
   "source": [
    "Next we implement a method for fitting (training) our model with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "rX9kl2kp4a1x"
   },
   "outputs": [],
   "source": [
    "def fit(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience):\n",
    "    \"\"\"\n",
    "    Fit the given model on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        lr_scheduler: Learning rate scheduler that improves training\n",
    "                      in late epochs with learning rate decay\n",
    "        dataloaders: Dataloaders for training and validation\n",
    "        max_epochs: Maximum number of epochs for training\n",
    "        patience: Number of epochs to wait with early stopping the\n",
    "                  training if validation loss has decreased\n",
    "                  \n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_acc = 0\n",
    "    curr_patience = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n",
    "        \n",
    "        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=False)\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n",
    "        \n",
    "        # Add early stopping and save the best weights (in best_model_weights)\n",
    "        if np.isclose(best_acc,val_acc,0.5) is True:\n",
    "            curr_patience += 1\n",
    "        best_acc = val_acc\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        if curr_patience >= patience:\n",
    "          #get best model weights but how\n",
    "          break\n",
    "\n",
    "\n",
    "    model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0CJAjbtp4a1y",
    "outputId": "6b62bdb5-0963-4544-bc01-c77048b46eb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/200, train loss: 1.67e-02, accuracy: 31.50%\n",
      "Epoch   1/200, val loss: 1.69e-02, accuracy: 34.48%\n",
      "Epoch   2/200, train loss: 1.61e-02, accuracy: 39.15%\n",
      "Epoch   2/200, val loss: 1.64e-02, accuracy: 40.24%\n",
      "Epoch   3/200, train loss: 1.59e-02, accuracy: 42.74%\n",
      "Epoch   3/200, val loss: 1.64e-02, accuracy: 40.96%\n",
      "Epoch   4/200, train loss: 1.57e-02, accuracy: 45.45%\n",
      "Epoch   4/200, val loss: 1.57e-02, accuracy: 49.42%\n",
      "Epoch   5/200, train loss: 1.54e-02, accuracy: 48.42%\n",
      "Epoch   5/200, val loss: 1.57e-02, accuracy: 50.30%\n",
      "Epoch   6/200, train loss: 1.53e-02, accuracy: 50.49%\n",
      "Epoch   6/200, val loss: 1.56e-02, accuracy: 50.88%\n",
      "Epoch   7/200, train loss: 1.51e-02, accuracy: 52.08%\n",
      "Epoch   7/200, val loss: 1.54e-02, accuracy: 52.92%\n",
      "Epoch   8/200, train loss: 1.51e-02, accuracy: 53.32%\n",
      "Epoch   8/200, val loss: 1.53e-02, accuracy: 53.92%\n",
      "Epoch   9/200, train loss: 1.50e-02, accuracy: 54.55%\n",
      "Epoch   9/200, val loss: 1.51e-02, accuracy: 56.90%\n",
      "Epoch  10/200, train loss: 1.48e-02, accuracy: 56.02%\n",
      "Epoch  10/200, val loss: 1.51e-02, accuracy: 57.88%\n",
      "Epoch  11/200, train loss: 1.48e-02, accuracy: 56.68%\n",
      "Epoch  11/200, val loss: 1.52e-02, accuracy: 56.28%\n",
      "Epoch  12/200, train loss: 1.47e-02, accuracy: 58.02%\n",
      "Epoch  12/200, val loss: 1.49e-02, accuracy: 59.84%\n",
      "Epoch  13/200, train loss: 1.46e-02, accuracy: 59.31%\n",
      "Epoch  13/200, val loss: 1.48e-02, accuracy: 60.86%\n",
      "Epoch  14/200, train loss: 1.46e-02, accuracy: 59.68%\n",
      "Epoch  14/200, val loss: 1.47e-02, accuracy: 62.02%\n",
      "Epoch  15/200, train loss: 1.45e-02, accuracy: 60.63%\n",
      "Epoch  15/200, val loss: 1.48e-02, accuracy: 61.72%\n",
      "Epoch  16/200, train loss: 1.44e-02, accuracy: 61.29%\n",
      "Epoch  16/200, val loss: 1.45e-02, accuracy: 64.34%\n",
      "Epoch  17/200, train loss: 1.44e-02, accuracy: 62.15%\n",
      "Epoch  17/200, val loss: 1.47e-02, accuracy: 62.44%\n",
      "Epoch  18/200, train loss: 1.43e-02, accuracy: 62.86%\n",
      "Epoch  18/200, val loss: 1.46e-02, accuracy: 63.82%\n",
      "Epoch  19/200, train loss: 1.42e-02, accuracy: 63.82%\n",
      "Epoch  19/200, val loss: 1.47e-02, accuracy: 62.56%\n",
      "Epoch  20/200, train loss: 1.43e-02, accuracy: 63.66%\n",
      "Epoch  20/200, val loss: 1.45e-02, accuracy: 65.28%\n",
      "Epoch  21/200, train loss: 1.42e-02, accuracy: 64.44%\n",
      "Epoch  21/200, val loss: 1.44e-02, accuracy: 65.66%\n",
      "Epoch  22/200, train loss: 1.42e-02, accuracy: 64.50%\n",
      "Epoch  22/200, val loss: 1.43e-02, accuracy: 67.96%\n",
      "Epoch  23/200, train loss: 1.42e-02, accuracy: 65.06%\n",
      "Epoch  23/200, val loss: 1.44e-02, accuracy: 65.62%\n",
      "Epoch  24/200, train loss: 1.41e-02, accuracy: 65.90%\n",
      "Epoch  24/200, val loss: 1.45e-02, accuracy: 64.12%\n",
      "Epoch  25/200, train loss: 1.40e-02, accuracy: 66.37%\n",
      "Epoch  25/200, val loss: 1.43e-02, accuracy: 67.00%\n",
      "Epoch  26/200, train loss: 1.40e-02, accuracy: 66.92%\n",
      "Epoch  26/200, val loss: 1.41e-02, accuracy: 70.74%\n",
      "Epoch  27/200, train loss: 1.40e-02, accuracy: 66.96%\n",
      "Epoch  27/200, val loss: 1.44e-02, accuracy: 65.94%\n",
      "Epoch  28/200, train loss: 1.39e-02, accuracy: 67.74%\n",
      "Epoch  28/200, val loss: 1.42e-02, accuracy: 69.32%\n",
      "Epoch  29/200, train loss: 1.39e-02, accuracy: 67.70%\n",
      "Epoch  29/200, val loss: 1.43e-02, accuracy: 66.94%\n",
      "Epoch  30/200, train loss: 1.39e-02, accuracy: 68.16%\n",
      "Epoch  30/200, val loss: 1.45e-02, accuracy: 65.68%\n",
      "Epoch  31/200, train loss: 1.39e-02, accuracy: 69.06%\n",
      "Epoch  31/200, val loss: 1.43e-02, accuracy: 68.04%\n",
      "Epoch  32/200, train loss: 1.39e-02, accuracy: 68.86%\n",
      "Epoch  32/200, val loss: 1.41e-02, accuracy: 69.78%\n",
      "Epoch  33/200, train loss: 1.38e-02, accuracy: 69.10%\n",
      "Epoch  33/200, val loss: 1.42e-02, accuracy: 68.34%\n",
      "Epoch  34/200, train loss: 1.38e-02, accuracy: 69.27%\n",
      "Epoch  34/200, val loss: 1.42e-02, accuracy: 69.20%\n",
      "Epoch  35/200, train loss: 1.38e-02, accuracy: 69.58%\n",
      "Epoch  35/200, val loss: 1.40e-02, accuracy: 70.34%\n",
      "Epoch  36/200, train loss: 1.38e-02, accuracy: 69.42%\n",
      "Epoch  36/200, val loss: 1.41e-02, accuracy: 70.02%\n",
      "Epoch  37/200, train loss: 1.38e-02, accuracy: 69.79%\n",
      "Epoch  37/200, val loss: 1.40e-02, accuracy: 71.36%\n",
      "Epoch  38/200, train loss: 1.38e-02, accuracy: 69.80%\n",
      "Epoch  38/200, val loss: 1.46e-02, accuracy: 63.36%\n",
      "Epoch  39/200, train loss: 1.38e-02, accuracy: 70.07%\n",
      "Epoch  39/200, val loss: 1.42e-02, accuracy: 69.30%\n",
      "Epoch  40/200, train loss: 1.37e-02, accuracy: 70.94%\n",
      "Epoch  40/200, val loss: 1.39e-02, accuracy: 72.12%\n",
      "Epoch  41/200, train loss: 1.37e-02, accuracy: 70.73%\n",
      "Epoch  41/200, val loss: 1.41e-02, accuracy: 69.98%\n",
      "Epoch  42/200, train loss: 1.37e-02, accuracy: 70.90%\n",
      "Epoch  42/200, val loss: 1.41e-02, accuracy: 70.18%\n",
      "Epoch  43/200, train loss: 1.37e-02, accuracy: 70.82%\n",
      "Epoch  43/200, val loss: 1.40e-02, accuracy: 71.36%\n",
      "Epoch  44/200, train loss: 1.37e-02, accuracy: 70.96%\n",
      "Epoch  44/200, val loss: 1.39e-02, accuracy: 73.48%\n",
      "Epoch  45/200, train loss: 1.37e-02, accuracy: 71.26%\n",
      "Epoch  45/200, val loss: 1.42e-02, accuracy: 68.34%\n",
      "Epoch  46/200, train loss: 1.37e-02, accuracy: 71.40%\n",
      "Epoch  46/200, val loss: 1.39e-02, accuracy: 72.66%\n",
      "Epoch  47/200, train loss: 1.37e-02, accuracy: 71.54%\n",
      "Epoch  47/200, val loss: 1.39e-02, accuracy: 72.98%\n",
      "Epoch  48/200, train loss: 1.36e-02, accuracy: 72.08%\n",
      "Epoch  48/200, val loss: 1.40e-02, accuracy: 71.44%\n",
      "Epoch  49/200, train loss: 1.37e-02, accuracy: 71.45%\n",
      "Epoch  49/200, val loss: 1.39e-02, accuracy: 72.62%\n",
      "Epoch  50/200, train loss: 1.36e-02, accuracy: 72.14%\n",
      "Epoch  50/200, val loss: 1.38e-02, accuracy: 74.72%\n",
      "Epoch  51/200, train loss: 1.36e-02, accuracy: 71.71%\n",
      "Epoch  51/200, val loss: 1.39e-02, accuracy: 72.32%\n",
      "Epoch  52/200, train loss: 1.36e-02, accuracy: 71.91%\n",
      "Epoch  52/200, val loss: 1.38e-02, accuracy: 73.94%\n",
      "Epoch  53/200, train loss: 1.36e-02, accuracy: 72.23%\n",
      "Epoch  53/200, val loss: 1.40e-02, accuracy: 71.84%\n",
      "Epoch  54/200, train loss: 1.36e-02, accuracy: 72.14%\n",
      "Epoch  54/200, val loss: 1.39e-02, accuracy: 72.54%\n",
      "Epoch  55/200, train loss: 1.36e-02, accuracy: 72.03%\n",
      "Epoch  55/200, val loss: 1.38e-02, accuracy: 74.04%\n",
      "Epoch  56/200, train loss: 1.36e-02, accuracy: 72.70%\n",
      "Epoch  56/200, val loss: 1.38e-02, accuracy: 73.44%\n",
      "Epoch  57/200, train loss: 1.35e-02, accuracy: 72.83%\n",
      "Epoch  57/200, val loss: 1.39e-02, accuracy: 72.58%\n",
      "Epoch  58/200, train loss: 1.36e-02, accuracy: 72.27%\n",
      "Epoch  58/200, val loss: 1.39e-02, accuracy: 72.68%\n",
      "Epoch  59/200, train loss: 1.36e-02, accuracy: 72.45%\n",
      "Epoch  59/200, val loss: 1.38e-02, accuracy: 73.70%\n",
      "Epoch  60/200, train loss: 1.36e-02, accuracy: 72.82%\n",
      "Epoch  60/200, val loss: 1.39e-02, accuracy: 72.72%\n",
      "Epoch  61/200, train loss: 1.36e-02, accuracy: 72.43%\n",
      "Epoch  61/200, val loss: 1.38e-02, accuracy: 74.48%\n",
      "Epoch  62/200, train loss: 1.35e-02, accuracy: 72.91%\n",
      "Epoch  62/200, val loss: 1.38e-02, accuracy: 73.48%\n",
      "Epoch  63/200, train loss: 1.35e-02, accuracy: 73.33%\n",
      "Epoch  63/200, val loss: 1.38e-02, accuracy: 72.92%\n",
      "Epoch  64/200, train loss: 1.35e-02, accuracy: 73.20%\n",
      "Epoch  64/200, val loss: 1.40e-02, accuracy: 71.74%\n",
      "Epoch  65/200, train loss: 1.35e-02, accuracy: 73.06%\n",
      "Epoch  65/200, val loss: 1.39e-02, accuracy: 72.44%\n",
      "Epoch  66/200, train loss: 1.35e-02, accuracy: 73.22%\n",
      "Epoch  66/200, val loss: 1.37e-02, accuracy: 74.56%\n",
      "Epoch  67/200, train loss: 1.35e-02, accuracy: 73.29%\n",
      "Epoch  67/200, val loss: 1.39e-02, accuracy: 72.84%\n",
      "Epoch  68/200, train loss: 1.35e-02, accuracy: 73.51%\n",
      "Epoch  68/200, val loss: 1.37e-02, accuracy: 75.04%\n",
      "Epoch  69/200, train loss: 1.35e-02, accuracy: 73.82%\n",
      "Epoch  69/200, val loss: 1.39e-02, accuracy: 72.72%\n",
      "Epoch  70/200, train loss: 1.35e-02, accuracy: 73.56%\n",
      "Epoch  70/200, val loss: 1.38e-02, accuracy: 73.76%\n",
      "Epoch  71/200, train loss: 1.35e-02, accuracy: 73.45%\n",
      "Epoch  71/200, val loss: 1.38e-02, accuracy: 73.48%\n",
      "Epoch  72/200, train loss: 1.35e-02, accuracy: 73.69%\n",
      "Epoch  72/200, val loss: 1.39e-02, accuracy: 72.14%\n",
      "Epoch  73/200, train loss: 1.35e-02, accuracy: 73.29%\n",
      "Epoch  73/200, val loss: 1.40e-02, accuracy: 71.42%\n",
      "Epoch  74/200, train loss: 1.35e-02, accuracy: 73.26%\n",
      "Epoch  74/200, val loss: 1.38e-02, accuracy: 74.22%\n",
      "Epoch  75/200, train loss: 1.35e-02, accuracy: 73.72%\n",
      "Epoch  75/200, val loss: 1.38e-02, accuracy: 73.90%\n",
      "Epoch  76/200, train loss: 1.34e-02, accuracy: 74.21%\n",
      "Epoch  76/200, val loss: 1.38e-02, accuracy: 74.36%\n",
      "Epoch  77/200, train loss: 1.35e-02, accuracy: 74.10%\n",
      "Epoch  77/200, val loss: 1.38e-02, accuracy: 73.26%\n",
      "Epoch  78/200, train loss: 1.34e-02, accuracy: 74.45%\n",
      "Epoch  78/200, val loss: 1.37e-02, accuracy: 74.50%\n",
      "Epoch  79/200, train loss: 1.34e-02, accuracy: 74.14%\n",
      "Epoch  79/200, val loss: 1.39e-02, accuracy: 72.88%\n",
      "Epoch  80/200, train loss: 1.34e-02, accuracy: 74.16%\n",
      "Epoch  80/200, val loss: 1.39e-02, accuracy: 72.38%\n",
      "Epoch  81/200, train loss: 1.35e-02, accuracy: 73.87%\n",
      "Epoch  81/200, val loss: 1.37e-02, accuracy: 75.00%\n",
      "Epoch  82/200, train loss: 1.34e-02, accuracy: 74.40%\n",
      "Epoch  82/200, val loss: 1.38e-02, accuracy: 73.84%\n",
      "Epoch  83/200, train loss: 1.34e-02, accuracy: 74.34%\n",
      "Epoch  83/200, val loss: 1.36e-02, accuracy: 76.00%\n",
      "Epoch  84/200, train loss: 1.34e-02, accuracy: 74.47%\n",
      "Epoch  84/200, val loss: 1.37e-02, accuracy: 74.86%\n",
      "Epoch  85/200, train loss: 1.34e-02, accuracy: 74.99%\n",
      "Epoch  85/200, val loss: 1.36e-02, accuracy: 76.12%\n",
      "Epoch  86/200, train loss: 1.34e-02, accuracy: 74.52%\n",
      "Epoch  86/200, val loss: 1.39e-02, accuracy: 73.28%\n",
      "Epoch  87/200, train loss: 1.34e-02, accuracy: 74.50%\n",
      "Epoch  87/200, val loss: 1.38e-02, accuracy: 74.32%\n",
      "Epoch  88/200, train loss: 1.34e-02, accuracy: 74.31%\n",
      "Epoch  88/200, val loss: 1.40e-02, accuracy: 71.48%\n",
      "Epoch  89/200, train loss: 1.34e-02, accuracy: 74.46%\n",
      "Epoch  89/200, val loss: 1.36e-02, accuracy: 76.28%\n",
      "Epoch  90/200, train loss: 1.34e-02, accuracy: 74.46%\n",
      "Epoch  90/200, val loss: 1.37e-02, accuracy: 75.06%\n",
      "Epoch  91/200, train loss: 1.34e-02, accuracy: 74.80%\n",
      "Epoch  91/200, val loss: 1.37e-02, accuracy: 74.98%\n",
      "Epoch  92/200, train loss: 1.34e-02, accuracy: 74.42%\n",
      "Epoch  92/200, val loss: 1.37e-02, accuracy: 75.52%\n",
      "Epoch  93/200, train loss: 1.34e-02, accuracy: 74.29%\n",
      "Epoch  93/200, val loss: 1.38e-02, accuracy: 73.80%\n",
      "Epoch  94/200, train loss: 1.34e-02, accuracy: 75.05%\n",
      "Epoch  94/200, val loss: 1.39e-02, accuracy: 73.08%\n",
      "Epoch  95/200, train loss: 1.34e-02, accuracy: 74.60%\n",
      "Epoch  95/200, val loss: 1.38e-02, accuracy: 73.56%\n",
      "Epoch  96/200, train loss: 1.34e-02, accuracy: 74.58%\n",
      "Epoch  96/200, val loss: 1.38e-02, accuracy: 74.20%\n",
      "Epoch  97/200, train loss: 1.34e-02, accuracy: 75.06%\n",
      "Epoch  97/200, val loss: 1.37e-02, accuracy: 74.72%\n",
      "Epoch  98/200, train loss: 1.34e-02, accuracy: 74.86%\n",
      "Epoch  98/200, val loss: 1.37e-02, accuracy: 74.46%\n",
      "Epoch  99/200, train loss: 1.34e-02, accuracy: 74.68%\n",
      "Epoch  99/200, val loss: 1.39e-02, accuracy: 73.32%\n",
      "Epoch 100/200, train loss: 1.34e-02, accuracy: 74.93%\n",
      "Epoch 100/200, val loss: 1.37e-02, accuracy: 75.04%\n",
      "Epoch 101/200, train loss: 1.31e-02, accuracy: 78.55%\n",
      "Epoch 101/200, val loss: 1.33e-02, accuracy: 80.42%\n",
      "Epoch 102/200, train loss: 1.30e-02, accuracy: 80.45%\n",
      "Epoch 102/200, val loss: 1.32e-02, accuracy: 81.22%\n",
      "Epoch 103/200, train loss: 1.29e-02, accuracy: 81.06%\n",
      "Epoch 103/200, val loss: 1.32e-02, accuracy: 81.40%\n",
      "Epoch 104/200, train loss: 1.29e-02, accuracy: 81.47%\n",
      "Epoch 104/200, val loss: 1.32e-02, accuracy: 81.60%\n",
      "Epoch 105/200, train loss: 1.29e-02, accuracy: 81.51%\n",
      "Epoch 105/200, val loss: 1.32e-02, accuracy: 82.22%\n",
      "Epoch 106/200, train loss: 1.28e-02, accuracy: 81.91%\n",
      "Epoch 106/200, val loss: 1.32e-02, accuracy: 82.08%\n",
      "Epoch 107/200, train loss: 1.28e-02, accuracy: 82.31%\n",
      "Epoch 107/200, val loss: 1.31e-02, accuracy: 82.52%\n",
      "Epoch 108/200, train loss: 1.28e-02, accuracy: 82.62%\n",
      "Epoch 108/200, val loss: 1.32e-02, accuracy: 82.36%\n",
      "Epoch 109/200, train loss: 1.28e-02, accuracy: 82.56%\n",
      "Epoch 109/200, val loss: 1.31e-02, accuracy: 82.48%\n",
      "Epoch 110/200, train loss: 1.28e-02, accuracy: 82.66%\n",
      "Epoch 110/200, val loss: 1.32e-02, accuracy: 82.56%\n",
      "Epoch 111/200, train loss: 1.28e-02, accuracy: 82.91%\n",
      "Epoch 111/200, val loss: 1.32e-02, accuracy: 82.66%\n",
      "Epoch 112/200, train loss: 1.28e-02, accuracy: 83.08%\n",
      "Epoch 112/200, val loss: 1.31e-02, accuracy: 82.76%\n",
      "Epoch 113/200, train loss: 1.27e-02, accuracy: 83.15%\n",
      "Epoch 113/200, val loss: 1.31e-02, accuracy: 82.78%\n",
      "Epoch 114/200, train loss: 1.27e-02, accuracy: 83.25%\n",
      "Epoch 114/200, val loss: 1.31e-02, accuracy: 82.72%\n",
      "Epoch 115/200, train loss: 1.27e-02, accuracy: 83.36%\n",
      "Epoch 115/200, val loss: 1.31e-02, accuracy: 82.86%\n",
      "Epoch 116/200, train loss: 1.27e-02, accuracy: 83.54%\n",
      "Epoch 116/200, val loss: 1.31e-02, accuracy: 82.96%\n",
      "Epoch 117/200, train loss: 1.27e-02, accuracy: 83.64%\n",
      "Epoch 117/200, val loss: 1.31e-02, accuracy: 83.06%\n",
      "Epoch 118/200, train loss: 1.27e-02, accuracy: 83.90%\n",
      "Epoch 118/200, val loss: 1.31e-02, accuracy: 83.36%\n",
      "Epoch 119/200, train loss: 1.27e-02, accuracy: 83.70%\n",
      "Epoch 119/200, val loss: 1.31e-02, accuracy: 83.12%\n",
      "Epoch 120/200, train loss: 1.27e-02, accuracy: 83.53%\n",
      "Epoch 120/200, val loss: 1.31e-02, accuracy: 83.32%\n",
      "Epoch 121/200, train loss: 1.27e-02, accuracy: 84.05%\n",
      "Epoch 121/200, val loss: 1.30e-02, accuracy: 83.76%\n",
      "Epoch 122/200, train loss: 1.27e-02, accuracy: 83.90%\n",
      "Epoch 122/200, val loss: 1.31e-02, accuracy: 83.10%\n",
      "Epoch 123/200, train loss: 1.27e-02, accuracy: 83.84%\n",
      "Epoch 123/200, val loss: 1.31e-02, accuracy: 83.22%\n",
      "Epoch 124/200, train loss: 1.27e-02, accuracy: 84.26%\n",
      "Epoch 124/200, val loss: 1.31e-02, accuracy: 82.96%\n",
      "Epoch 125/200, train loss: 1.27e-02, accuracy: 84.01%\n",
      "Epoch 125/200, val loss: 1.31e-02, accuracy: 83.22%\n",
      "Epoch 126/200, train loss: 1.26e-02, accuracy: 84.48%\n",
      "Epoch 126/200, val loss: 1.31e-02, accuracy: 83.44%\n",
      "Epoch 127/200, train loss: 1.27e-02, accuracy: 84.40%\n",
      "Epoch 127/200, val loss: 1.31e-02, accuracy: 83.42%\n",
      "Epoch 128/200, train loss: 1.27e-02, accuracy: 84.21%\n",
      "Epoch 128/200, val loss: 1.31e-02, accuracy: 83.22%\n",
      "Epoch 129/200, train loss: 1.27e-02, accuracy: 84.36%\n",
      "Epoch 129/200, val loss: 1.31e-02, accuracy: 83.22%\n",
      "Epoch 130/200, train loss: 1.26e-02, accuracy: 84.63%\n",
      "Epoch 130/200, val loss: 1.31e-02, accuracy: 83.26%\n",
      "Epoch 131/200, train loss: 1.26e-02, accuracy: 84.66%\n",
      "Epoch 131/200, val loss: 1.31e-02, accuracy: 83.24%\n",
      "Epoch 132/200, train loss: 1.26e-02, accuracy: 85.03%\n",
      "Epoch 132/200, val loss: 1.31e-02, accuracy: 83.12%\n",
      "Epoch 133/200, train loss: 1.26e-02, accuracy: 84.62%\n",
      "Epoch 133/200, val loss: 1.30e-02, accuracy: 83.56%\n",
      "Epoch 134/200, train loss: 1.26e-02, accuracy: 84.90%\n",
      "Epoch 134/200, val loss: 1.31e-02, accuracy: 83.20%\n",
      "Epoch 135/200, train loss: 1.26e-02, accuracy: 84.89%\n",
      "Epoch 135/200, val loss: 1.31e-02, accuracy: 82.90%\n",
      "Epoch 136/200, train loss: 1.26e-02, accuracy: 84.81%\n",
      "Epoch 136/200, val loss: 1.30e-02, accuracy: 83.48%\n",
      "Epoch 137/200, train loss: 1.26e-02, accuracy: 84.95%\n",
      "Epoch 137/200, val loss: 1.30e-02, accuracy: 84.04%\n",
      "Epoch 138/200, train loss: 1.26e-02, accuracy: 85.15%\n",
      "Epoch 138/200, val loss: 1.30e-02, accuracy: 83.58%\n",
      "Epoch 139/200, train loss: 1.26e-02, accuracy: 84.98%\n",
      "Epoch 139/200, val loss: 1.31e-02, accuracy: 83.26%\n",
      "Epoch 140/200, train loss: 1.26e-02, accuracy: 85.11%\n",
      "Epoch 140/200, val loss: 1.30e-02, accuracy: 83.52%\n",
      "Epoch 141/200, train loss: 1.26e-02, accuracy: 85.27%\n",
      "Epoch 141/200, val loss: 1.30e-02, accuracy: 83.62%\n",
      "Epoch 142/200, train loss: 1.26e-02, accuracy: 85.07%\n",
      "Epoch 142/200, val loss: 1.31e-02, accuracy: 83.08%\n",
      "Epoch 143/200, train loss: 1.26e-02, accuracy: 85.31%\n",
      "Epoch 143/200, val loss: 1.31e-02, accuracy: 83.14%\n",
      "Epoch 144/200, train loss: 1.26e-02, accuracy: 85.29%\n",
      "Epoch 144/200, val loss: 1.31e-02, accuracy: 83.54%\n",
      "Epoch 145/200, train loss: 1.26e-02, accuracy: 85.29%\n",
      "Epoch 145/200, val loss: 1.31e-02, accuracy: 83.42%\n",
      "Epoch 146/200, train loss: 1.26e-02, accuracy: 85.55%\n",
      "Epoch 146/200, val loss: 1.31e-02, accuracy: 83.48%\n",
      "Epoch 147/200, train loss: 1.26e-02, accuracy: 85.31%\n",
      "Epoch 147/200, val loss: 1.30e-02, accuracy: 83.64%\n",
      "Epoch 148/200, train loss: 1.26e-02, accuracy: 85.50%\n",
      "Epoch 148/200, val loss: 1.31e-02, accuracy: 82.96%\n",
      "Epoch 149/200, train loss: 1.26e-02, accuracy: 85.37%\n",
      "Epoch 149/200, val loss: 1.31e-02, accuracy: 82.84%\n",
      "Epoch 150/200, train loss: 1.26e-02, accuracy: 85.29%\n",
      "Epoch 150/200, val loss: 1.30e-02, accuracy: 83.60%\n",
      "Epoch 151/200, train loss: 1.25e-02, accuracy: 85.95%\n",
      "Epoch 151/200, val loss: 1.30e-02, accuracy: 84.20%\n",
      "Epoch 152/200, train loss: 1.25e-02, accuracy: 86.46%\n",
      "Epoch 152/200, val loss: 1.30e-02, accuracy: 84.30%\n",
      "Epoch 153/200, train loss: 1.25e-02, accuracy: 86.38%\n",
      "Epoch 153/200, val loss: 1.30e-02, accuracy: 84.26%\n",
      "Epoch 154/200, train loss: 1.25e-02, accuracy: 86.51%\n",
      "Epoch 154/200, val loss: 1.30e-02, accuracy: 84.10%\n",
      "Epoch 155/200, train loss: 1.25e-02, accuracy: 86.59%\n",
      "Epoch 155/200, val loss: 1.30e-02, accuracy: 84.28%\n",
      "Epoch 156/200, train loss: 1.25e-02, accuracy: 86.78%\n",
      "Epoch 156/200, val loss: 1.30e-02, accuracy: 83.96%\n",
      "Epoch 157/200, train loss: 1.25e-02, accuracy: 86.70%\n",
      "Epoch 157/200, val loss: 1.30e-02, accuracy: 84.26%\n",
      "Epoch 158/200, train loss: 1.25e-02, accuracy: 86.71%\n",
      "Epoch 158/200, val loss: 1.30e-02, accuracy: 84.02%\n",
      "Epoch 159/200, train loss: 1.24e-02, accuracy: 87.06%\n",
      "Epoch 159/200, val loss: 1.30e-02, accuracy: 84.12%\n",
      "Epoch 160/200, train loss: 1.25e-02, accuracy: 86.86%\n",
      "Epoch 160/200, val loss: 1.30e-02, accuracy: 84.42%\n",
      "Epoch 161/200, train loss: 1.25e-02, accuracy: 86.95%\n",
      "Epoch 161/200, val loss: 1.30e-02, accuracy: 84.18%\n",
      "Epoch 162/200, train loss: 1.24e-02, accuracy: 86.99%\n",
      "Epoch 162/200, val loss: 1.30e-02, accuracy: 84.16%\n",
      "Epoch 163/200, train loss: 1.25e-02, accuracy: 86.88%\n",
      "Epoch 163/200, val loss: 1.30e-02, accuracy: 84.28%\n",
      "Epoch 164/200, train loss: 1.24e-02, accuracy: 87.05%\n",
      "Epoch 164/200, val loss: 1.30e-02, accuracy: 84.26%\n",
      "Epoch 165/200, train loss: 1.24e-02, accuracy: 87.11%\n",
      "Epoch 165/200, val loss: 1.30e-02, accuracy: 84.28%\n",
      "Epoch 166/200, train loss: 1.24e-02, accuracy: 87.13%\n",
      "Epoch 166/200, val loss: 1.30e-02, accuracy: 84.26%\n",
      "Epoch 167/200, train loss: 1.24e-02, accuracy: 87.08%\n",
      "Epoch 167/200, val loss: 1.30e-02, accuracy: 84.32%\n",
      "Epoch 168/200, train loss: 1.24e-02, accuracy: 87.05%\n",
      "Epoch 168/200, val loss: 1.30e-02, accuracy: 84.42%\n",
      "Epoch 169/200, train loss: 1.24e-02, accuracy: 87.07%\n",
      "Epoch 169/200, val loss: 1.30e-02, accuracy: 84.18%\n",
      "Epoch 170/200, train loss: 1.24e-02, accuracy: 87.21%\n",
      "Epoch 170/200, val loss: 1.30e-02, accuracy: 84.20%\n",
      "Epoch 171/200, train loss: 1.24e-02, accuracy: 87.11%\n",
      "Epoch 171/200, val loss: 1.30e-02, accuracy: 84.08%\n",
      "Epoch 172/200, train loss: 1.24e-02, accuracy: 87.09%\n",
      "Epoch 172/200, val loss: 1.30e-02, accuracy: 84.14%\n",
      "Epoch 173/200, train loss: 1.24e-02, accuracy: 87.31%\n",
      "Epoch 173/200, val loss: 1.30e-02, accuracy: 84.32%\n",
      "Epoch 174/200, train loss: 1.24e-02, accuracy: 87.34%\n",
      "Epoch 174/200, val loss: 1.30e-02, accuracy: 84.12%\n",
      "Epoch 175/200, train loss: 1.24e-02, accuracy: 87.26%\n",
      "Epoch 175/200, val loss: 1.30e-02, accuracy: 84.34%\n",
      "Epoch 176/200, train loss: 1.24e-02, accuracy: 87.22%\n",
      "Epoch 176/200, val loss: 1.30e-02, accuracy: 84.24%\n",
      "Epoch 177/200, train loss: 1.24e-02, accuracy: 87.34%\n",
      "Epoch 177/200, val loss: 1.30e-02, accuracy: 84.32%\n",
      "Epoch 178/200, train loss: 1.24e-02, accuracy: 87.35%\n",
      "Epoch 178/200, val loss: 1.30e-02, accuracy: 84.34%\n",
      "Epoch 179/200, train loss: 1.24e-02, accuracy: 87.38%\n",
      "Epoch 179/200, val loss: 1.30e-02, accuracy: 84.46%\n",
      "Epoch 180/200, train loss: 1.24e-02, accuracy: 87.39%\n",
      "Epoch 180/200, val loss: 1.30e-02, accuracy: 84.12%\n",
      "Epoch 181/200, train loss: 1.24e-02, accuracy: 87.19%\n",
      "Epoch 181/200, val loss: 1.30e-02, accuracy: 84.22%\n",
      "Epoch 182/200, train loss: 1.24e-02, accuracy: 87.64%\n",
      "Epoch 182/200, val loss: 1.30e-02, accuracy: 84.02%\n",
      "Epoch 183/200, train loss: 1.24e-02, accuracy: 87.51%\n",
      "Epoch 183/200, val loss: 1.30e-02, accuracy: 84.34%\n",
      "Epoch 184/200, train loss: 1.24e-02, accuracy: 87.48%\n",
      "Epoch 184/200, val loss: 1.30e-02, accuracy: 84.22%\n",
      "Epoch 185/200, train loss: 1.24e-02, accuracy: 87.46%\n",
      "Epoch 185/200, val loss: 1.30e-02, accuracy: 84.04%\n",
      "Epoch 186/200, train loss: 1.24e-02, accuracy: 87.55%\n",
      "Epoch 186/200, val loss: 1.30e-02, accuracy: 84.20%\n",
      "Epoch 187/200, train loss: 1.24e-02, accuracy: 87.48%\n",
      "Epoch 187/200, val loss: 1.30e-02, accuracy: 84.44%\n",
      "Epoch 188/200, train loss: 1.24e-02, accuracy: 87.38%\n",
      "Epoch 188/200, val loss: 1.30e-02, accuracy: 84.24%\n",
      "Epoch 189/200, train loss: 1.24e-02, accuracy: 87.45%\n",
      "Epoch 189/200, val loss: 1.30e-02, accuracy: 84.26%\n",
      "Epoch 190/200, train loss: 1.24e-02, accuracy: 87.21%\n",
      "Epoch 190/200, val loss: 1.30e-02, accuracy: 84.22%\n",
      "Epoch 191/200, train loss: 1.24e-02, accuracy: 87.60%\n",
      "Epoch 191/200, val loss: 1.30e-02, accuracy: 84.38%\n",
      "Epoch 192/200, train loss: 1.24e-02, accuracy: 87.57%\n",
      "Epoch 192/200, val loss: 1.30e-02, accuracy: 84.14%\n",
      "Epoch 193/200, train loss: 1.24e-02, accuracy: 87.39%\n",
      "Epoch 193/200, val loss: 1.30e-02, accuracy: 84.46%\n",
      "Epoch 194/200, train loss: 1.24e-02, accuracy: 87.57%\n",
      "Epoch 194/200, val loss: 1.30e-02, accuracy: 84.36%\n",
      "Epoch 195/200, train loss: 1.24e-02, accuracy: 87.70%\n",
      "Epoch 195/200, val loss: 1.30e-02, accuracy: 84.36%\n",
      "Epoch 196/200, train loss: 1.24e-02, accuracy: 87.53%\n",
      "Epoch 196/200, val loss: 1.30e-02, accuracy: 84.44%\n",
      "Epoch 197/200, train loss: 1.24e-02, accuracy: 87.50%\n",
      "Epoch 197/200, val loss: 1.30e-02, accuracy: 84.50%\n",
      "Epoch 198/200, train loss: 1.24e-02, accuracy: 87.73%\n",
      "Epoch 198/200, val loss: 1.30e-02, accuracy: 84.32%\n",
      "Epoch 199/200, train loss: 1.24e-02, accuracy: 87.49%\n",
      "Epoch 199/200, val loss: 1.30e-02, accuracy: 84.36%\n",
      "Epoch 200/200, train loss: 1.24e-02, accuracy: 87.71%\n",
      "Epoch 200/200, val loss: 1.30e-02, accuracy: 84.40%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-41d6a0615e44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-107-27f6e391c5e6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model_weights' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# Fit model\n",
    "fit(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=200, patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEl7JWVL4a1z"
   },
   "source": [
    "Once the model is trained we run it on the test set to obtain our final accuracy.\n",
    "Note that we can only look at the test set once, everything else would lead to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uy0_9V1L4a1z"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
    "print(f\"Test loss: {test_loss:.1e}, accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeCpm92p4a10"
   },
   "source": [
    "That's almost what was reported in the paper (92.49%) and we didn't even train on the full training set."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "exercise_08_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
